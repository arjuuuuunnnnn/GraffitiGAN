{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download -d arbazkhan971/cuhk-face-sketch-database-cufs\n",
        "! unzip cuhk-face-sketch-database-cufs.zip"
      ],
      "metadata": {
        "id": "iodmEr_F318g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI5fizYo3jzq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 3\n",
        "LATENT_DIM = 128\n",
        "VAE_LR = 1e-4\n",
        "GEN_LR = 1e-4\n",
        "DISC_LR = 1e-4\n",
        "LAMBDA_CYCLE = 10\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Directories\n",
        "REAL_IMAGES_DIR = '/content/photos'\n",
        "PENCIL_SKETCHES_DIR = '/content/sketches'\n",
        "\n",
        "# Transformation for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])"
      ],
      "metadata": {
        "id": "-iU1jH4J3ySX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sorted_alphanumeric(data):\n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
        "    return sorted(data, key=alphanum_key)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, real_images_path, sketches_path, transform=None):\n",
        "        # Get all file paths\n",
        "        self.real_images = glob.glob(real_images_path)\n",
        "        self.sketches = glob.glob(sketches_path)\n",
        "\n",
        "        # Sort file paths\n",
        "        self.real_images = sorted_alphanumeric(self.real_images)\n",
        "        self.sketches = sorted_alphanumeric(self.sketches)\n",
        "\n",
        "        # Ensure that lengths of real_images and sketches match\n",
        "        if len(self.real_images) != len(self.sketches):\n",
        "            raise ValueError(\"The number of real images and sketches must be the same\")\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.real_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_image = Image.open(self.real_images[idx])\n",
        "        sketch = Image.open(self.sketches[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            real_image = self.transform(real_image)\n",
        "            sketch = self.transform(sketch)\n",
        "\n",
        "        return real_image, sketch"
      ],
      "metadata": {
        "id": "e5EcrSNR3_6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(nn.Module):\n",
        "    def forward(self, z_mean, z_log_var):\n",
        "        epsilon = torch.randn_like(z_mean)\n",
        "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 16)\n",
        "        self.fc2_mean = nn.Linear(16, latent_dim)\n",
        "        self.fc2_log_var = nn.Linear(16, latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        z_mean = self.fc2_mean(x)\n",
        "        z_log_var = self.fc2_log_var(x)\n",
        "        z = self.sampling(z_mean, z_log_var)\n",
        "        return z_mean, z_log_var, z\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, 64 * 32 * 32)\n",
        "        self.fc2 = nn.Linear(64 * 32 * 32, 3 * 128 * 128)\n",
        "        self.deconv1 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = torch.relu(self.fc1(z))\n",
        "        x = x.view(x.size(0), 64, 32, 32)\n",
        "        x = torch.relu(self.deconv1(x))\n",
        "        x = torch.sigmoid(self.deconv2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "8CgkNIAd4T6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.down_stack = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.up_stack = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.final_layer = nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1, activation='tanh')\n",
        "\n",
        "    def forward(self, x):\n",
        "        down = self.down_stack(x)\n",
        "        x = self.up_stack(down)\n",
        "        return self.final_layer(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "_kInp8Gj4qkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(real, reconstruction, z_mean, z_log_var):\n",
        "    recon_loss = nn.functional.mse_loss(reconstruction, real, reduction='sum')\n",
        "    kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
        "    return recon_loss + kl_loss\n",
        "\n",
        "def train_vae(vae, dataloader, optimizer):\n",
        "    vae.train()\n",
        "    total_loss = 0\n",
        "    for real_images, _ in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        z_mean, z_log_var, z = vae.encoder(real_images)\n",
        "        reconstruction = vae.decoder(z)\n",
        "        loss = vae_loss(real_images, reconstruction, z_mean, z_log_var)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def gan_loss_fn(real, fake):\n",
        "    return nn.functional.binary_cross_entropy_with_logits(fake, real)\n",
        "\n",
        "def train_gan(gan, dataloader, vae_optimizer, gen_optimizer, disc_optimizer):\n",
        "    gan.train()\n",
        "    total_gen_loss = 0\n",
        "    total_disc_loss = 0\n",
        "    for real_images, sketches in dataloader:\n",
        "        # VAE training\n",
        "        vae_loss = train_vae(gan.vae, dataloader, vae_optimizer)\n",
        "\n",
        "        # GAN training\n",
        "        real_images = real_images.to(device)\n",
        "        sketches = sketches.to(device)\n",
        "\n",
        "        fake_sketches = gan.generator(real_images)\n",
        "        cycled_images = gan.generator(fake_sketches)\n",
        "        same_sketches = gan.generator(sketches)\n",
        "\n",
        "        disc_real_sketches = gan.discriminator(sketches)\n",
        "        disc_fake_sketches = gan.discriminator(fake_sketches)\n",
        "\n",
        "        valid = torch.ones_like(disc_fake_sketches)\n",
        "        fake = torch.zeros_like(disc_fake_sketches)\n",
        "\n",
        "        gen_loss = gan_loss_fn(valid, disc_fake_sketches) + \\\n",
        "                   LAMBDA_CYCLE * nn.functional.l1_loss(real_images, cycled_images) + \\\n",
        "                   LAMBDA_CYCLE * nn.functional.l1_loss(sketches, same_sketches)\n",
        "\n",
        "        disc_loss = (gan_loss_fn(valid, disc_real_sketches) + \\\n",
        "                     gan_loss_fn(fake, disc_fake_sketches)) * 0.5\n",
        "\n",
        "        gen_optimizer.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        gen_optimizer.step()\n",
        "\n",
        "        disc_optimizer.zero_grad()\n",
        "        disc_loss.backward()\n",
        "        disc_optimizer.step()\n",
        "\n",
        "        total_gen_loss += gen_loss.item()\n",
        "        total_disc_loss += disc_loss.item()\n",
        "\n",
        "    return total_gen_loss / len(dataloader), total_disc_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "mWltN0sz4uo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = ImageDataset(\n",
        "    real_images_path=os.path.join(REAL_IMAGES_DIR, '*'),\n",
        "    sketches_path=os.path.join(PENCIL_SKETCHES_DIR, '*'),\n",
        "    transform=transform\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "encoder = Encoder(LATENT_DIM).to(device)\n",
        "decoder = Decoder(LATENT_DIM).to(device)\n",
        "vae = VAE(encoder, decoder).to(device)\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Optimizers\n",
        "vae_optimizer = optim.Adam(vae.parameters(), lr=VAE_LR)\n",
        "gen_optimizer = optim.Adam(generator.parameters(), lr=GEN_LR)\n",
        "disc_optimizer = optim.Adam(discriminator.parameters(), lr=DISC_LR)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    vae_loss = train_vae(vae, dataloader, vae_optimizer)\n",
        "    gen_loss, disc_loss = train_gan(gan, dataloader, vae_optimizer, gen_optimizer, disc_optimizer)\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS} - VAE Loss: {vae_loss:.4f} - Gen Loss: {gen_loss:.4f} - Disc Loss: {disc_loss:.4f}')"
      ],
      "metadata": {
        "id": "bgb2Cti04yNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}